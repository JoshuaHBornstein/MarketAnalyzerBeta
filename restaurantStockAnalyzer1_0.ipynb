{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r6ONvoaZpaUm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%matplotlib inline\n",
        "\n",
        "# For reading stock data from yahoo\n",
        "from pandas_datareader.data import DataReader\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "yf.pdr_override()\n",
        "\n",
        "# For time stamps\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "end = datetime.now()\n",
        "start = datetime(end.year - 1, end.month, end.day)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create list of restaurant ticker symbols called ticker symbols\n",
        "ticker_list = ['MCD', 'CMG', 'YUM', 'QSR', 'DRI', 'YUMC', 'DPZ', 'WING', '6862.HK', 'TXRH', 'CAVA', '2702.T', 'JBFCF', 'SHAK', 'WEN', 'DMP.AX', 'PLAY', 'ARCO', 'BLMN', 'PZZA', 'DEVYANI.NS', 'SSPG.L', 'EAT', 'CAKE', 'SG', 'FWRG', 'JACK', 'EAT.MC', 'KRUS', 'MTY.TO', 'BJRI', 'PTLO', '9658.HK', 'DIN', '0341.HK', 'CHUY', 'BH', 'TAST', 'DENN', 'PBPB', '2753.TW', 'PZA.TO', 'LOCO', 'NATH', 'BARBEQUE.NS', 'GENK', 'RBD.NZ', 'THCH', 'FAT', '0052.HK', 'STKS', 'RRGB', 'NDLS', 'RAVE']\n",
        "#removed 'AMR.AE' and 'RTN.L'\n",
        "all_data = pd.DataFrame()\n",
        "# Define a function to calculate RSI\n",
        "def calculate_rsi(data, window=14):\n",
        "    delta = data.diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Define a function to calculate SMA\n",
        "def calculate_sma(data, window=20):\n",
        "    return data.rolling(window=window).mean()\n",
        "\n",
        "for ticker in ticker_list:\n",
        "    # Fetch the daily data\n",
        "    daily_data = yf.download(ticker, start=start, end=end)[['Close', 'Volume']].copy()\n",
        "    daily_data.reset_index(inplace=True)\n",
        "\n",
        "    daily_data['RSI'] = calculate_rsi(daily_data['Close'])\n",
        "    daily_data['SMA_20'] = calculate_sma(daily_data['Close'])\n",
        "\n",
        "    # Add lagged features for 'Close' and 'Volume'\n",
        "    daily_data['Close_lag1'] = daily_data['Close'].shift(1)\n",
        "    daily_data['Volume_lag1'] = daily_data['Volume'].shift(1)\n",
        "\n",
        "    # Fetch the market cap and target price (mean)\n",
        "    ticker_obj = yf.Ticker(ticker)\n",
        "    info = ticker_obj.info\n",
        "    market_cap = info.get('marketCap', None)\n",
        "    target_mean_price = info.get('targetMeanPrice', None)\n",
        "\n",
        "    # Add the ticker, market cap, and target mean price to the daily data\n",
        "    daily_data['Ticker'] = ticker\n",
        "    daily_data['Market_Cap'] = market_cap\n",
        "    daily_data['Target_Mean_Price'] = target_mean_price\n",
        "\n",
        "    # Append to the main DataFrame\n",
        "    all_data = pd.concat([all_data, daily_data])\n",
        "\n",
        "# Convert 'Ticker' column to one-hot encoded columns\n",
        "#all_data = pd.get_dummies(all_data, columns=['Ticker'])\n",
        "\n",
        "print(all_data.shape)\n",
        "print(all_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1snSlL52_Url",
        "outputId": "a69e0128-e387-427b-a706-330cfdfb6eec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13363, 10)\n",
            "        Date       Close   Volume  RSI  SMA_20  Close_lag1  Volume_lag1  \\\n",
            "0 2023-03-06  270.640015  2217600  NaN     NaN         NaN          NaN   \n",
            "1 2023-03-07  267.130005  2766600  NaN     NaN  270.640015    2217600.0   \n",
            "2 2023-03-08  265.329987  2314000  NaN     NaN  267.130005    2766600.0   \n",
            "3 2023-03-09  261.630005  2339300  NaN     NaN  265.329987    2314000.0   \n",
            "4 2023-03-10  262.029999  3093100  NaN     NaN  261.630005    2339300.0   \n",
            "\n",
            "  Ticker    Market_Cap  Target_Mean_Price  \n",
            "0    MCD  210073796608             327.05  \n",
            "1    MCD  210073796608             327.05  \n",
            "2    MCD  210073796608             327.05  \n",
            "3    MCD  210073796608             327.05  \n",
            "4    MCD  210073796608             327.05  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Feature Preparation\n",
        "# Drop rows with missing values to simplify the example\n",
        "all_data.dropna(inplace=True)\n",
        "\n",
        "# Step 2: Label Creation\n",
        "# Shift the 'Close' price to create the target variable: 1 if the price goes up, else 0\n",
        "all_data['Target'] = (all_data.groupby('Ticker')['Close'].shift(-1) > all_data['Close']).astype(int)\n",
        "\n",
        "# Filter out the last row for each ticker since it won't have a label\n",
        "all_data = all_data.groupby('Ticker').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
        "\n",
        "all_data = pd.get_dummies(all_data, columns=['Ticker'])\n",
        "\n",
        "# Assuming we are focusing on one stock for simplicity, filter by ticker if desired\n",
        "# all_data = all_data[all_data['Ticker'] == 'AAPL']\n",
        "\n",
        "# Separate features and target variable\n",
        "X = all_data[['RSI', 'SMA_20', 'Close_lag1', 'Volume_lag1']]\n",
        "y = all_data['Target']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Model Training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpG6ySLk_avk",
        "outputId": "fae22024-3fbb-4b6a-b321-2276300f44f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5122067975107707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "feature_columns = ['RSI', 'SMA_20', 'Close_lag1', 'Volume_lag1']\n",
        "X = scaler.fit_transform(all_data[feature_columns])\n",
        "y = all_data['Target'].values\n",
        "\n",
        "# Sequential Split\n",
        "# Let's say we use the last 60 days as test data\n",
        "train_size = int(len(X) * 0.95)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "xwyGA2jf_s1q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATDI6_Vk_xWy",
        "outputId": "16e9c7aa-491e-4c1a-b6b9-7152c4015169"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 5ms/step - loss: 0.6941 - accuracy: 0.5191 - val_loss: 0.6947 - val_accuracy: 0.4980\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5326 - val_loss: 0.6936 - val_accuracy: 0.5005\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5298 - val_loss: 0.6927 - val_accuracy: 0.5066\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5245 - val_loss: 0.6927 - val_accuracy: 0.5121\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5306 - val_loss: 0.6930 - val_accuracy: 0.5161\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5307 - val_loss: 0.6921 - val_accuracy: 0.5121\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5355 - val_loss: 0.6953 - val_accuracy: 0.5005\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5348 - val_loss: 0.6933 - val_accuracy: 0.5176\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5341 - val_loss: 0.6939 - val_accuracy: 0.5015\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5308 - val_loss: 0.6959 - val_accuracy: 0.4924\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5345 - val_loss: 0.6966 - val_accuracy: 0.4889\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5338 - val_loss: 0.6929 - val_accuracy: 0.5166\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5343 - val_loss: 0.6955 - val_accuracy: 0.4980\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5341 - val_loss: 0.6958 - val_accuracy: 0.4945\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5348 - val_loss: 0.6947 - val_accuracy: 0.4975\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5361 - val_loss: 0.6941 - val_accuracy: 0.5076\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5357 - val_loss: 0.6932 - val_accuracy: 0.5040\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6907 - accuracy: 0.5369 - val_loss: 0.6943 - val_accuracy: 0.5020\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6909 - accuracy: 0.5359 - val_loss: 0.6928 - val_accuracy: 0.5156\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6907 - accuracy: 0.5328 - val_loss: 0.6963 - val_accuracy: 0.4945\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.5366 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5366 - val_loss: 0.6947 - val_accuracy: 0.5126\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6907 - accuracy: 0.5346 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5360 - val_loss: 0.6933 - val_accuracy: 0.5066\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5352 - val_loss: 0.6954 - val_accuracy: 0.5010\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5371 - val_loss: 0.6929 - val_accuracy: 0.5106\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5403 - val_loss: 0.6953 - val_accuracy: 0.5035\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5359 - val_loss: 0.6960 - val_accuracy: 0.5045\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5360 - val_loss: 0.6937 - val_accuracy: 0.5091\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5360 - val_loss: 0.6938 - val_accuracy: 0.5096\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6934 - val_accuracy: 0.5217\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5338 - val_loss: 0.6944 - val_accuracy: 0.5010\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5365 - val_loss: 0.6938 - val_accuracy: 0.5050\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5379 - val_loss: 0.6941 - val_accuracy: 0.5086\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5396 - val_loss: 0.6946 - val_accuracy: 0.5066\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5384 - val_loss: 0.6948 - val_accuracy: 0.5091\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5351 - val_loss: 0.6941 - val_accuracy: 0.5151\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5374 - val_loss: 0.6936 - val_accuracy: 0.5076\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5355 - val_loss: 0.6942 - val_accuracy: 0.5116\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5393 - val_loss: 0.6937 - val_accuracy: 0.5101\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5381 - val_loss: 0.6930 - val_accuracy: 0.5156\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5375 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5390 - val_loss: 0.6943 - val_accuracy: 0.5131\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5393 - val_loss: 0.6935 - val_accuracy: 0.5156\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5372 - val_loss: 0.6943 - val_accuracy: 0.5086\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.5386 - val_loss: 0.6956 - val_accuracy: 0.5096\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6941 - val_accuracy: 0.5121\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5347 - val_loss: 0.6933 - val_accuracy: 0.5181\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5350 - val_loss: 0.6948 - val_accuracy: 0.5091\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5388 - val_loss: 0.6942 - val_accuracy: 0.5076\n",
            "Test Loss: 0.6940279603004456, Test Accuracy: 0.4894837439060211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYXToXVx_zjd"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}